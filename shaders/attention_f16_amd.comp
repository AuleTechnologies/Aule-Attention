#version 450
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#extension GL_EXT_shader_16bit_storage : require
#extension GL_KHR_shader_subgroup_arithmetic : require

// FlashAttention-2 Forward Pass - AMD FP16 Optimized Version
// Optimized for AMD RDNA 2/3 GPUs with native FP16 support
//
// Performance benefits:
// - 2x compute throughput vs FP32
// - 2x memory bandwidth (half the data to move)
// - Native FP16 ALUs on RDNA 2/3
//
// Supported GPUs:
// - RX 6000 series (RDNA 2) - Native FP16
// - RX 7000 series (RDNA 3) - Native FP16 + packed math
// - Radeon 680M/780M APUs - Native FP16
// - Steam Deck (Van Gogh APU) - RDNA 2

// AMD wavefront size - use 64 for GCN compat, 32 for pure RDNA wave32
layout(local_size_x = 64, local_size_y = 1) in;

// Storage buffers with FP16
layout(set = 0, binding = 0) readonly buffer QueryBuffer {
    float16_t data[];
} Q;

layout(set = 0, binding = 1) readonly buffer KeyBuffer {
    float16_t data[];
} K;

layout(set = 0, binding = 2) readonly buffer ValueBuffer {
    float16_t data[];
} V;

layout(set = 0, binding = 3) writeonly buffer OutputBuffer {
    float16_t data[];
} O;

// Push constants for dimensions
layout(push_constant) uniform PushConstants {
    uint batch_size;    // B
    uint num_heads;     // H
    uint seq_len;       // N
    uint head_dim;      // D (must be <= 128)
    float scale;        // 1/sqrt(D), kept as float for precision
    uint causal;        // 1 for causal masking (LLMs), 0 for bidirectional
} params;

// Shared memory with FP16 - 2x capacity vs FP32
const uint TILE_SIZE = 64;

shared float16_t s_K[TILE_SIZE][128];
shared float16_t s_V[TILE_SIZE][128];

void main() {
    uint lane_id = gl_LocalInvocationID.x;

    // Global position
    uint batch_head_idx = gl_WorkGroupID.z;
    uint batch_idx = batch_head_idx / params.num_heads;
    uint head_idx = batch_head_idx % params.num_heads;
    uint query_row = gl_WorkGroupID.y;

    bool is_active = (batch_idx < params.batch_size) && (query_row < params.seq_len);

    uint base_offset = (batch_idx * params.num_heads + head_idx) * params.seq_len * params.head_dim;
    uint actual_head_dim = min(params.head_dim, 128u);

    // Load Q values - each thread handles up to 2 dimensions
    float16_t q_vals[2];
    q_vals[0] = float16_t(0.0);
    q_vals[1] = float16_t(0.0);

    if (is_active && lane_id < actual_head_dim) {
        q_vals[0] = Q.data[base_offset + query_row * params.head_dim + lane_id];
    }
    if (is_active && lane_id + 64 < actual_head_dim) {
        q_vals[1] = Q.data[base_offset + query_row * params.head_dim + lane_id + 64];
    }

    // Use FP32 for accumulation to maintain precision
    float row_max = -1e30;
    float row_sum = 0.0;
    float output_acc[2] = float[2](0.0, 0.0);

    uint num_kv_blocks = (params.seq_len + TILE_SIZE - 1) / TILE_SIZE;
    float16_t scale_fp16 = float16_t(params.scale);

    for (uint kv_block = 0; kv_block < num_kv_blocks; kv_block++) {
        uint kv_start = kv_block * TILE_SIZE;
        uint kv_row = kv_start + lane_id;
        bool kv_valid = kv_row < params.seq_len;

        // Load K/V tiles
        for (uint d = 0; d < actual_head_dim; d++) {
            if (kv_valid && batch_idx < params.batch_size) {
                s_K[lane_id][d] = K.data[base_offset + kv_row * params.head_dim + d];
                s_V[lane_id][d] = V.data[base_offset + kv_row * params.head_dim + d];
            } else {
                s_K[lane_id][d] = float16_t(0.0);
                s_V[lane_id][d] = float16_t(0.0);
            }
        }

        barrier();

        float block_max = -1e30;
        float scores[64];

        for (uint k = 0; k < TILE_SIZE; k++) {
            uint global_k = kv_start + k;

            // FP16 dot product with FP32 accumulation for precision
            float partial_score = 0.0;
            if (lane_id < actual_head_dim) {
                partial_score = float(q_vals[0]) * float(s_K[k][lane_id]);
            }
            if (lane_id + 64 < actual_head_dim) {
                partial_score += float(q_vals[1]) * float(s_K[k][lane_id + 64]);
            }

            float score = subgroupAdd(partial_score);
            score *= params.scale;

            // Mask out-of-bounds positions AND apply causal mask
            bool is_masked = !is_active || global_k >= params.seq_len;

            // Causal masking: mask positions where key_pos > query_pos
            if (params.causal != 0u && global_k > query_row) {
                is_masked = true;
            }

            if (is_masked) {
                score = -1e30;
            }

            scores[k] = score;
            block_max = max(block_max, score);
        }

        if (is_active) {
            float new_max = max(row_max, block_max);
            float old_scale_factor = exp(row_max - new_max);

            row_sum *= old_scale_factor;
            output_acc[0] *= old_scale_factor;
            output_acc[1] *= old_scale_factor;

            float block_sum = 0.0;
            for (uint k = 0; k < TILE_SIZE; k++) {
                float p = exp(scores[k] - new_max);
                block_sum += p;

                // Accumulate V with FP16 loads, FP32 accumulation
                if (lane_id < actual_head_dim) {
                    output_acc[0] += p * float(s_V[k][lane_id]);
                }
                if (lane_id + 64 < actual_head_dim) {
                    output_acc[1] += p * float(s_V[k][lane_id + 64]);
                }
            }

            row_sum += block_sum;
            row_max = new_max;
        }

        barrier();
    }

    // Final output in FP16
    if (is_active) {
        float inv_sum = 1.0 / row_sum;

        if (lane_id < actual_head_dim) {
            O.data[base_offset + query_row * params.head_dim + lane_id] = float16_t(output_acc[0] * inv_sum);
        }
        if (lane_id + 64 < actual_head_dim) {
            O.data[base_offset + query_row * params.head_dim + lane_id + 64] = float16_t(output_acc[1] * inv_sum);
        }
    }
}
