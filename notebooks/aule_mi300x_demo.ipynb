{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aule-attention on AMD MI300X\n",
    "\n",
    "This notebook demonstrates aule-attention running on AMD Instinct MI300X GPU.\n",
    "\n",
    "**Requirements:**\n",
    "- PyTorch with ROCm support\n",
    "- aule-attention (`pip install aule-attention`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install aule-attention if not already installed\n",
    "!pip install aule-attention -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import aule-attention and check backends\n",
    "from aule import flash_attention, get_available_backends, print_backend_info\n",
    "\n",
    "print(\"Available backends:\", get_available_backends())\n",
    "print()\n",
    "print_backend_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Inference\n",
    "\n",
    "Simple forward pass demonstrating FlashAttention on MI300X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inference example\n",
    "batch_size = 1\n",
    "num_heads = 32\n",
    "seq_len = 2048\n",
    "head_dim = 128\n",
    "\n",
    "# Create random tensors (simulating transformer hidden states)\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "\n",
    "print(f\"Input shapes: Q={q.shape}, K={k.shape}, V={v.shape}\")\n",
    "print(f\"Dtype: {q.dtype}\")\n",
    "print(f\"Device: {q.device}\")\n",
    "\n",
    "# Run attention\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "output = flash_attention(q, k, v, causal=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Time: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Numerical Accuracy Test\n",
    "\n",
    "Compare aule-attention against PyTorch's scaled_dot_product_attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison with PyTorch SDPA\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 2\n",
    "num_heads = 8\n",
    "seq_len = 512\n",
    "head_dim = 64\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float32)\n",
    "\n",
    "# aule-attention\n",
    "out_aule = flash_attention(q, k, v, causal=True)\n",
    "\n",
    "# PyTorch SDPA\n",
    "out_torch = torch.nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "# Compare\n",
    "diff = (out_aule - out_torch).abs()\n",
    "print(f\"Maximum absolute difference: {diff.max().item():.2e}\")\n",
    "print(f\"Mean absolute difference: {diff.mean().item():.2e}\")\n",
    "print(f\"Relative error: {(diff / out_torch.abs().clamp(min=1e-6)).max().item():.2e}\")\n",
    "print(f\"\\nOutputs match: {torch.allclose(out_aule, out_torch, atol=1e-4, rtol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Backward Pass\n",
    "\n",
    "Demonstrate gradient computation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training example with gradients\n",
    "batch_size = 4\n",
    "num_heads = 16\n",
    "seq_len = 1024\n",
    "head_dim = 64\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16, requires_grad=True)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16, requires_grad=True)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16, requires_grad=True)\n",
    "\n",
    "print(f\"Input shapes: Q={q.shape}\")\n",
    "print(f\"requires_grad: Q={q.requires_grad}, K={k.requires_grad}, V={v.requires_grad}\")\n",
    "\n",
    "# Forward pass\n",
    "output = flash_attention(q, k, v, causal=True)\n",
    "\n",
    "# Compute loss (simple sum for demonstration)\n",
    "loss = output.sum()\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed:\")\n",
    "print(f\"  dQ shape: {q.grad.shape}, norm: {q.grad.norm().item():.4f}\")\n",
    "print(f\"  dK shape: {k.grad.shape}, norm: {k.grad.norm().item():.4f}\")\n",
    "print(f\"  dV shape: {v.grad.shape}, norm: {v.grad.norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grouped Query Attention (GQA)\n",
    "\n",
    "Modern LLMs like Llama 2/3 use GQA where query heads > key/value heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Query Attention (Llama-style)\n",
    "batch_size = 1\n",
    "num_q_heads = 32   # Query heads\n",
    "num_kv_heads = 8   # Key/Value heads (4:1 ratio)\n",
    "seq_len = 2048\n",
    "head_dim = 128\n",
    "\n",
    "q = torch.randn(batch_size, num_q_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(batch_size, num_kv_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(batch_size, num_kv_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "\n",
    "print(f\"GQA Configuration:\")\n",
    "print(f\"  Query heads: {num_q_heads}\")\n",
    "print(f\"  KV heads: {num_kv_heads}\")\n",
    "print(f\"  Ratio: {num_q_heads // num_kv_heads}:1\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "output = flash_attention(q, k, v, causal=True)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(f\"Time: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmark\n",
    "\n",
    "Measure throughput at different sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(batch_size, num_heads, seq_len, head_dim, num_warmup=5, num_runs=20):\n",
    "    \"\"\"Benchmark attention throughput.\"\"\"\n",
    "    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(num_warmup):\n",
    "        _ = flash_attention(q, k, v, causal=True)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        _ = flash_attention(q, k, v, causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # ms\n",
    "    \n",
    "    # Calculate TFLOPS (approximate)\n",
    "    # FLOPs for attention: 4 * batch * heads * seq^2 * head_dim (for Q@K^T and attn@V)\n",
    "    flops = 4 * batch_size * num_heads * seq_len * seq_len * head_dim\n",
    "    tflops = flops / (avg_time / 1000) / 1e12\n",
    "    \n",
    "    return avg_time, tflops\n",
    "\n",
    "print(\"Performance Benchmark (B=1, H=32, D=128)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Seq Len':<12} {'Time (ms)':<12} {'TFLOPS':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for seq_len in [512, 1024, 2048, 4096, 8192]:\n",
    "    try:\n",
    "        avg_time, tflops = benchmark_attention(\n",
    "            batch_size=1,\n",
    "            num_heads=32,\n",
    "            seq_len=seq_len,\n",
    "            head_dim=128\n",
    "        )\n",
    "        print(f\"{seq_len:<12} {avg_time:<12.2f} {tflops:<12.1f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{seq_len:<12} Error: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simple Transformer Block\n",
    "\n",
    "A minimal transformer attention block using aule-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AuleAttentionBlock(nn.Module):\n",
    "    \"\"\"Simple attention block using aule-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention using aule\n",
    "        attn_out = flash_attention(q, k, v, causal=True)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_dim)\n",
    "        return self.o_proj(attn_out)\n",
    "\n",
    "# Create and test the block\n",
    "hidden_dim = 2048\n",
    "num_heads = 16\n",
    "batch_size = 2\n",
    "seq_len = 1024\n",
    "\n",
    "model = AuleAttentionBlock(hidden_dim, num_heads).cuda().half()\n",
    "x = torch.randn(batch_size, seq_len, hidden_dim, device='cuda', dtype=torch.float16)\n",
    "\n",
    "print(f\"Model: AuleAttentionBlock\")\n",
    "print(f\"  Hidden dim: {hidden_dim}\")\n",
    "print(f\"  Num heads: {num_heads}\")\n",
    "print(f\"  Head dim: {hidden_dim // num_heads}\")\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "output = model(x)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Time: {elapsed*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop Example\n",
    "\n",
    "A minimal training loop with the attention block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training loop\n",
    "model = AuleAttentionBlock(hidden_dim=1024, num_heads=8).cuda().half()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 512\n",
    "hidden_dim = 1024\n",
    "num_steps = 10\n",
    "\n",
    "print(\"Training loop with aule-attention\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Generate random input\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Simple loss (MSE to zero for demonstration)\n",
    "    loss = output.pow(2).mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Step {step+1:2d}: loss = {loss.item():.6f}\")\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Usage\n",
    "\n",
    "Check GPU memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage test\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "batch_size = 1\n",
    "num_heads = 32\n",
    "seq_len = 8192\n",
    "head_dim = 128\n",
    "\n",
    "q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda', dtype=torch.float16)\n",
    "\n",
    "input_memory = torch.cuda.memory_allocated() / 1e9\n",
    "print(f\"Memory after input allocation: {input_memory:.2f} GB\")\n",
    "\n",
    "output = flash_attention(q, k, v, causal=True)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "total_memory = torch.cuda.memory_allocated() / 1e9\n",
    "peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "\n",
    "print(f\"Memory after attention: {total_memory:.2f} GB\")\n",
    "print(f\"Peak memory: {peak_memory:.2f} GB\")\n",
    "print(f\"\\nNote: FlashAttention uses O(N) memory, not O(N^2)\")\n",
    "print(f\"Traditional attention at seq_len={seq_len} would need ~{batch_size * num_heads * seq_len * seq_len * 2 / 1e9:.1f} GB for attention matrix alone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "aule-attention provides:\n",
    "- Drop-in FlashAttention for AMD MI300X\n",
    "- Full training support with backward pass\n",
    "- GQA/MQA support for modern LLMs\n",
    "- O(N) memory complexity\n",
    "- No compilation required at install time\n",
    "\n",
    "Install: `pip install aule-attention`\n",
    "\n",
    "Documentation: https://github.com/xenn0010/Aule-Attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
